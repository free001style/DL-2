{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Краткий отчёт\n","\n","Так как теперь нет блока с LayerNorm и ReLU, то линейный слой, skip connection и умножение на маску можно немного переписать:  теперь не нужно добавлять размерность и менять оси. Можно умножить маску на skip connection, на линейный слой и на bias. Правда, поэлементное умножение и сложение работает медленнее, чем einsum, поэтому можно переписать всё через него. Также можно пойти дальше и засунуть матрицу из 1 в линейный слой (ну и не менять эту матрицу).\n","\n","Дальше я добавил amp, compile и  ddp. Ещё увеличил батч, чтобы гпу забивалось почти полностью.\n","\n","По итогу теперь за одну эпоху проходит 79 * 2048 * 2 / 34 = 9.5k объектов в секунду."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-30T17:01:26.984093Z","iopub.status.busy":"2024-10-30T17:01:26.983713Z","iopub.status.idle":"2024-10-30T17:01:45.910002Z","shell.execute_reply":"2024-10-30T17:01:45.909119Z","shell.execute_reply.started":"2024-10-30T17:01:26.984039Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting triton\n","  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton) (3.15.1)\n","Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: triton\n","Successfully installed triton-3.1.0\n"]}],"source":["!pip install triton"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-10-30T17:21:19.091363Z","iopub.status.busy":"2024-10-30T17:21:19.090934Z","iopub.status.idle":"2024-10-30T17:21:19.102620Z","shell.execute_reply":"2024-10-30T17:21:19.101703Z","shell.execute_reply.started":"2024-10-30T17:21:19.091320Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting ddp.py\n"]}],"source":["%%writefile ddp.py\n","# > A slow and inefficient implementation of a slightly modified Trompt model\n","# > From the ICLM 2023 paper https://arxiv.org/abs/2305.18446\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.data\n","\n","import os\n","import urllib.request\n","from tqdm import tqdm\n","import torch.distributed as dist\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","        \n","\n","\n","class TromptCell(nn.Module):\n","    def __init__(self, n_columns, n_prompts, d_model):\n","        super().__init__()\n","        # Embeddings (Figure 3.2)\n","        self.feature_emb_weight = nn.Parameter(torch.empty(n_columns, d_model))\n","        self.feature_emb_bias = nn.Parameter(torch.empty(n_columns, d_model))\n","        self.ln_emb = nn.LayerNorm(d_model)\n","\n","        # Importance Getter (Figure 3.1)\n","        self.ln_col = nn.LayerNorm(d_model)\n","        self.ln_prompt = nn.LayerNorm(d_model)\n","        self.dense_imp = nn.Linear(2 * d_model, d_model)\n","\n","        self.emb_column = nn.Parameter(torch.empty(n_columns, d_model))\n","        self.emb_prompt = nn.Parameter(torch.empty(n_prompts, d_model))\n","\n","        # Modified expansion block (Figure 3.3)\n","        # Without non-linearities! This is important to make significant speed-ups possible.\n","        self.dense_expand = nn.Linear(1, n_prompts)\n","        \n","        self.delta = torch.ones(1, n_prompts) / n_prompts\n","        self.delta.requires_grad_(False)\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        d_rsqrt = self.feature_emb_weight.shape[1] ** -0.5\n","        nn.init.uniform_(self.feature_emb_weight, -d_rsqrt, d_rsqrt)\n","        nn.init.uniform_(self.feature_emb_bias, -d_rsqrt, d_rsqrt)\n","        nn.init.normal_(self.emb_column, std=0.01)\n","        nn.init.normal_(self.emb_prompt, std=0.01)\n","\n","    def forward(self, x: torch.Tensor, prev_cell_out: torch.Tensor) -> torch.Tensor:\n","        x_emb = x.unsqueeze(-1) * self.feature_emb_weight + self.feature_emb_bias.unsqueeze(0)\n","        x_emb = F.relu(x_emb)\n","        x_emb = self.ln_emb(x_emb) \n","\n","        x_prompt = self.emb_prompt.unsqueeze(0)\n","        x_prompt = self.dense_imp(torch.cat([self.ln_prompt(x_prompt), prev_cell_out], dim=-1)) + x_prompt\n","        x_column = self.ln_col(self.emb_column.unsqueeze(0))\n","        mask = torch.softmax(torch.einsum(\"ijk, ilk -> ijl\", x_prompt, x_column), dim=-1)\n","\n","        x_out = torch.einsum('ijk, ikl, jm->ijl', mask, x_emb, self.delta.to(self.dense_expand.weight.device) + self.dense_expand.weight)\n","        x_out += torch.einsum('ijk, j->ij', mask, self.dense_expand.bias)\n","\n","        return x_out\n","\n","\n","class TromptDownstream(nn.Module):\n","    def __init__(self, d_model):\n","        super().__init__()\n","        self.dense0 = nn.Linear(d_model, 1)\n","        self.dense1 = nn.Linear(d_model, d_model)\n","        self.ln = nn.LayerNorm(d_model)\n","        self.dense_out = nn.Linear(d_model, 1)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        pw = torch.softmax(self.dense0(x).squeeze(-1), dim=-1)\n","        xnew = torch.einsum(\"ij, ijk -> ik\", pw, x)\n","        return self.dense_out(self.ln(F.relu(self.dense1(xnew))))\n","\n","\n","class Trompt(nn.Module):\n","    def __init__(self, n_columns, n_prompts, d_model, n_cycles):\n","        super().__init__()\n","        self.tcells = nn.ModuleList([TromptCell(n_columns, n_prompts, d_model) for _ in range(n_cycles)])\n","        self.tdown = TromptDownstream(d_model)\n","        self.prompt = nn.Parameter(torch.empty(n_prompts, d_model))\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        nn.init.normal_(self.prompt, std=0.01)\n","        \n","    def forward(self, x):\n","            x_prompt = self.prompt.unsqueeze(0)\n","            outputs = []\n","            for cell in self.tcells:\n","                outputs.append(self.tdown(cell(x, x_prompt)))\n","            return torch.stack(outputs, dim=1).squeeze(-1)\n","\n","\n","def load_from_url(url, cache_dir='.'):\n","    filename = os.path.join(cache_dir, url.split('/')[-1])\n","    if not os.path.exists(filename):\n","        with tqdm(unit='B', unit_scale=True, desc=filename) as pbar:\n","            urllib.request.urlretrieve(url, filename, reporthook=lambda _, b, t: pbar.update(b))\n","    return torch.load(filename, map_location=torch.device('cpu'), weights_only=True)\n","\n","\n","TRAIN_DATA = \"https://huggingface.co/datasets/puhsu/hw01-data/resolve/main/train_dataset.pt\"\n","VAL_DATA = \"https://huggingface.co/datasets/puhsu/hw01-data/resolve/main/val_dataset.pt\"\n","\n","def main(rank, world_size):\n","    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n","    torch.manual_seed(0)\n","    \n","    train_dataset = torch.utils.data.TensorDataset(*map(torch.nan_to_num, load_from_url(TRAIN_DATA)))\n","    val_dataset = torch.utils.data.TensorDataset(*map(torch.nan_to_num, load_from_url(VAL_DATA)))\n","\n","    Y_mean = train_dataset.tensors[1].mean()\n","    Y_std = train_dataset.tensors[1].std()\n","    train_dataset.tensors = (train_dataset.tensors[0], (train_dataset.tensors[1] - Y_mean) / Y_std)\n","\n","    model = Trompt(n_columns=train_dataset.tensors[0].shape[1], n_prompts=128, d_model=128, n_cycles=6)\n","    device = torch.device(f'cuda:{rank}')\n","    model.to(device)\n","    model = DDP(model, device_ids=[rank])\n","    model = torch.compile(model)\n","\n","    train_dl = torch.utils.data.DataLoader(train_dataset, num_workers=0, batch_size=2048, shuffle=True)\n","    val_dl = torch.utils.data.DataLoader(val_dataset, num_workers=0, batch_size=2048)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)\n","\n","    EPOCHS = 5\n","    scaler = torch.GradScaler()\n","\n","    for e in range(1, EPOCHS + 1):\n","        model.train()\n","        for batch in tqdm(train_dl):\n","            x, y = batch\n","            optimizer.zero_grad()\n","            with torch.autocast(device_type='cuda', enabled=True):\n","                pred = model(x.to(device))\n","                loss = F.mse_loss(pred, y.unsqueeze(1).repeat(1, len(model.module.tcells)).to(device))\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","        \n","        if rank == 0:\n","            model.eval()\n","            mae = 0\n","            with torch.inference_mode():\n","                for batch in tqdm(val_dl):\n","                    x, y = batch\n","                    pred = model(x.to(device))\n","                    true_pred = pred.clone()\n","                    pred = model(x.to(device))\n","                    mae += (pred.mean(dim=-1) * Y_std + Y_mean - y.to(device)).abs().sum().item()\n","\n","                mae = mae / len(val_dataset)\n","\n","                print(f'>>> Epoch {e:>02}')\n","                print(f'Validation MAE = {mae:.5f}')\n","                print('>>>\\n')\n","    dist.destroy_process_group()\n","    \n","if __name__ == \"__main__\":\n","    world_size = torch.cuda.device_count()\n","    torch.multiprocessing.spawn(main, args=(world_size,), nprocs=world_size, join=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-30T17:27:03.642269Z","iopub.status.busy":"2024-10-30T17:27:03.641851Z","iopub.status.idle":"2024-10-30T17:30:18.908191Z","shell.execute_reply":"2024-10-30T17:30:18.906887Z","shell.execute_reply.started":"2024-10-30T17:27:03.642230Z"},"trusted":true},"outputs":[],"source":["!python -W ignore -m torch.distributed.launch ddp.py"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
